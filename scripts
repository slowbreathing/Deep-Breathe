Installation Requirements
tensorflow(only for comparisons)
python 3.5 or above
numpy.

#LSTMs
mkdir resources/tmp/rnn_words
python org/mk/training/dl/LSTMAllByitself/LSTMMain.py input/belling_the_cat10.txt

python org/mk/training/dl/tfwordslstmsingle.py
python -m org.mk.training.dl.LSTMMainsingle

python -m org.mk.training.dl.tfwordslstm input/belling_the_cat10.txt
python -m org.mk.training.dl.LSTMMain input/belling_the_cat10.txt
python -m org.mk.training.dl.LSTMMainGraph input/belling_the_cat10.txt

python -m org.mk.training.dl.tfwordslstminitstate input/belling_the_cat10.txt
python -m org.mk.training.dl.LSTMMaininitstate input/belling_the_cat10.txt
python -m org.mk.training.dl.LSTMMainGraphinitstate input/belling_the_cat10.txt

python -m org.mk.training.dl.tfwordslstmmulti input/belling_the_cat10.txt
python -m org.mk.training.dl.LSTMMainmulti input/belling_the_cat10.txt
python -m org.mk.training.dl.LSTMMainGraphmulti input/belling_the_cat10.txt

python -m org.mk.training.dl.tfwordslstmbi input/belling_the_cat10.txt
python -m org.mk.training.dl.LSTMMainbi input/belling_the_cat10.txt
python -m org.mk.training.dl.LSTMMainGraphbi input/belling_the_cat10.txt

python -m org.mk.training.dl.tfwordslstmbimulti input/belling_the_cat10.txt
python -m org.mk.training.dl.LSTMMainbimulti input/belling_the_cat10.txt
python -m org.mk.training.dl.LSTMMainGraphbimulti input/belling_the_cat10.txt

#Neural Machine Translation(NMT)
python -m org.mk.training.dl.nmtdata

python -m org.mk.training.dl.tfmachinetranslation --encoder_type=uni --num_layers=1
python -m org.mk.training.dl.manualmachinetranslation --encoder_type=uni --num_layer=1
python -m org.mk.training.dl.manualmachinetranslationgraph --encoder_type=uni --num_layer=1

python -m org.mk.training.dl.tfmachinetranslation --encoder_type=uni --num_layers=2
python -m org.mk.training.dl.manualmachinetranslation --encoder_type=uni --num_layer=2
python -m org.mk.training.dl.manualmachinetranslationgraph --encoder_type=uni --num_layer=2

python -m org.mk.training.dl.tfmachinetranslation --num_layers=2
python -m org.mk.training.dl.manualmachinetranslation --num_layer=2
python -m org.mk.training.dl.manualmachinetranslationgraph --num_layer=2

python -m org.mk.training.dl.tfmachinetranslation --num_layers=4
python -m org.mk.training.dl.manualmachinetranslation --num_layer=4
python -m org.mk.training.dl.manualmachinetranslationgraph --num_layer=4

#Neural Machine Translation(NMT) with Attention
python -m org.mk.training.dl.tfmachinetranslation --attention_architecture=standard --encoder_type=uni --num_layers=1 --batch_size=1 
python -m org.mk.training.dl.manualmachinetranslation --attention_architecture=standard --encoder_type=uni --num_layers=1 --batch_size=1 
python -m org.mk.training.dl.manualmachinetranslationgraph --attention_architecture=standard --encoder_type=uni --num_layers=1 --batch_size=1 

python -m org.mk.training.dl.tfmachinetranslation --attention_architecture=standard --encoder_type=uni --num_layers=1 
python -m org.mk.training.dl.manualmachinetranslation --attention_architecture=standard --encoder_type=uni --num_layers=1
python -m org.mk.training.dl.manualmachinetranslationgraph --attention_architecture=standard --encoder_type=uni --num_layers=1

python -m org.mk.training.dl.tfmachinetranslation --attention_architecture=standard --encoder_type=uni
python -m org.mk.training.dl.manualmachinetranslation --attention_architecture=standard --encoder_type=uni
python -m org.mk.training.dl.manualmachinetranslationgraph --attention_architecture=standard --encoder_type=uni

python -m org.mk.training.dl.tfmachinetranslation --attention_architecture=standard 
python -m org.mk.training.dl.manualmachinetranslation --attention_architecture=standard
python -m org.mk.training.dl.manualmachinetranslationgraph --attention_architecture=standard

python -m org.mk.training.dl.tfmachinetranslation --attention_architecture=standard --num_layers=4 
python -m org.mk.training.dl.manualmachinetranslation --attention_architecture=standard --num_layers=4 
python -m org.mk.training.dl.manualmachinetranslationgraph --attention_architecture=standard --num_layers=4 
